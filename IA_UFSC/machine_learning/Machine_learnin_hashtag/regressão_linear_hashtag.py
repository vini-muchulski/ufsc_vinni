# -*- coding: utf-8 -*-
"""regressão linear hashtag

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18LOXQZpATE5eVZ9aetEMMkLWGgbF9xwn

# A Regressão Linear
- Se temos um conjunto de pontos como o mostrado abaixo, podemos traçar qualquer reta que passa por esses pontos
    - Nesse caso, vamos traçar uma reta y = x
<img src="https://drive.google.com/uc?id=1MAvsj8-PiAjITiZ5CLznoJAii9fANYxN" style="width: 400px;"/>
- Infinitas retas podem ser traçadas, mas qual seria <font color='blue'>**a melhor reta**</font> que passa por esses pontos?
<img src="https://drive.google.com/uc?id=1Da2PIitlCgvH1YLOYucImVZtffjQwbwv" style="width: 400px;"/>
- <font color='blue'>**a melhor reta:**</font> o que seria "melhor"?
    - Para definir esse conceito podemos, por exemplo, verificar a distância de cada ponto a essa reta vermelha e escolher baseado nessa distância
<img src="https://drive.google.com/uc?id=14p9ICVUjch8wXoMuz2rIaxK2_7_VwewI" style="width: 500px;"/>
- A **regressão vai traçar essa reta de forma a <font color='blue'>minimizar a soma dos erros ao quadrado**</font>, segundo a própria documentação
    - https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares
<img src="https://drive.google.com/uc?id=1hs6fc1WHOk6eF66MfarlmwGhxtWurUQb" style="width: 400px;"/>
"""

# Podemos considerar esses dados abaixo
import pandas as pd

dados = pd.DataFrame({
    'X': [1,2,3,4,5],
    'Y': [1.3,1.8,3.5,4,4.6]
})

dados.head(2)

dados.head(3)

# Nessa reta vermelha, fizemos que y = x, então podemos escrever o y_reta como
dados['y_reta'] = dados.X

# Visualizando esses pontos graficamente, podemos traçar uma reta que passa por esse pontos
import matplotlib.pyplot as plt

fig,ax = plt.subplots()

ax.scatter(dados.X,dados.Y)
ax.plot(dados.X,dados.y_reta,'--r')
ax.scatter(dados.X,dados.y_reta)

plt.show()

"""- **Vamos usar a regressão linear para traçar a melhor reta que passa por esses pontos**
    - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression
"""

# Importando a regressão linear
from sklearn.linear_model import LinearRegression

# Criando o regressor
reg = LinearRegression().fit(dados.X.values.reshape(-1, 1),dados.Y)

# Visualizando o coeficiente angular
a = reg.coef_[0]

reg.coef_

# e o coeficiente linear
b = reg.intercept_

# Visualizando graficamente
fig,ax = plt.subplots()

ax.scatter(dados.X,dados.Y)
ax.plot(dados.X,dados.y_reta,'--r')

x = dados.X.values
y = a*x+b
ax.plot(x,y)

plt.show()

# Fazendo a previsão e adicionando na base
dados['y_pred'] = reg.predict(dados.X.values.reshape(-1,1))

# Calculando o erro da reta vermelha e da regressão
dados['erro_reta'] = (dados.Y - dados.y_reta)**2
dados['erro_pred'] = (dados.Y - dados.y_pred)**2

# Verificando essa base e a soma do erro
dados[['erro_reta','erro_pred']].mean()

dados

"""- Pdemos utilizar o erro médio absoluto e o erro médio quadrático do próprio scikit-learn para calcular esses erros
    - https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics
"""

# Além disso, também podemos usar o erro médio absoluto e o erro médio quadrático do próprio scikit-learn
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

# Visualizando o resultado da previsão
print(mean_absolute_error(dados.Y,dados.y_reta))
print(mean_squared_error(dados.Y,dados.y_reta))
print(mean_absolute_error(dados.Y,dados.y_pred))
print(mean_squared_error(dados.Y,dados.y_pred))

"""### Usando a Regressão Linear de forma prática
- **Vamos utilizar o dataset de casas da Califórnia**
    - https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html
"""

# Importando o dataset
from sklearn.datasets import fetch_california_housing
data = fetch_california_housing()

# Visualizando
data

# Transformando em um DataFrame
casas = pd.DataFrame(data.data)
casas.columns = data.feature_names
casas['MedHouseVal'] = data.target

# Visualizando o dataframe
casas

"""- **Regressão Linear Simples**"""

# Separando X e Y
X = casas.MedInc
y = casas.MedHouseVal

"""- Separando nossa base em treino e teste para criarmos o modelo
    - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
"""

# Separando em treino e teste
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# Usando a regressão linear
reg = LinearRegression().fit(X_train.values.reshape(-1, 1),y_train)

# Avaliando o erro
y_pred = reg.predict(X_test.values.reshape(-1,1))

print(mean_absolute_error(y_test,y_pred))
print(mean_squared_error(y_test,y_pred))

# Visualizando a relação do y_test com o y_pred
fig,ax = plt.subplots()

ax.scatter(y_pred,y_test)
ax.plot([1,5],[1,5],'--r')

plt.show()

"""<a id='linear_multipla'></a>
- **Regressão Linear Múltipla**

- Primeiro fazendo para apenas duas variáveis
"""

# Separando X e Y
X = casas.drop('MedHouseVal',axis=1)
y = casas.MedHouseVal

# Separando em treino e teste
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# Usando a regressão linear
reg = LinearRegression().fit(X_train,y_train)

# Da mesma forma, também vamos ter o coef_
reg.coef_

# E o intercept_
reg.intercept_

# Avaliando o erro
y_pred = reg.predict(X_test)

print(mean_absolute_error(y_test,y_pred))
print(mean_squared_error(y_test,y_pred))

# Visualizando a relação do y_test com o y_pred
fig,ax = plt.subplots()

ax.scatter(y_pred,y_test)
ax.plot([1,5],[1,5],'--r')

plt.show()

"""- Podemos [voltar](#linear_multipla) e considerar qualquer coluna para o modelo"""