# -*- coding: utf-8 -*-
"""Regressão Logística hashtag

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KrnT58Q7RM7oHqosOo7p54hQzgVhtRpA

# A Regressão Logística
- É um modelo linear para **classificação** usando uma função dada por:
<img src="https://drive.google.com/uc?id=1YXiCkxBJ0vf8WQRjLgiw6uI-Xp2T56cZ" style='width: 300px;' />
- Onde, **para calcular s, vamos usar os mesmos conceitos que falamos na regressão linear (coef, intercept)**
<br><br>
- Na regressão logística, ao invés de determinar um valor binário para a classe (0 ou 1, maça ou banana, fraude ou não), ela retorna **a probabilidade de um evento ocorrer**
    - Como ele calcula probabilidades, é muito usado em problemas de classificação de crédito, previsão de saída de clientes (churn) e até probabilidade de doenças
<br><br>
- Considerando uma única variável (petal width (cm)) para fazer a previsão do dataset iris (que já falamos), podemos visualizar primeiramente os dados e então traçar essa função logística
<img src="https://drive.google.com/uc?id=1aSA66H2y8tiS2oVDz05BHgUhM218DcnL" style='width: 600px;' />
- Com isso, podemos fazer a previsão para qualquer novo valor
<img src="https://drive.google.com/uc?id=1lIvPbsn2PD0fPe-6m-HZnItzVh_gM0Oj" style='width: 2000px;' />

- **Para começar, podemos já usar o dataset iris**
    - https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris
"""

# Importando o dataset
from sklearn.datasets import load_iris
X,y = load_iris(return_X_y = True, as_frame=True)

# Considerando apenas a coluna 'petal width (cm)' e os targets 0 e 1
X = X.loc[y.isin([0,1]),'petal width (cm)'].values
y = y[y.isin([0,1])].values

"""- E separar em treino e teste
    - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
"""

# Separando em treino e teste
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

"""- Visualizando graficamente a relação do petal width (cm) com a classe
    - https://matplotlib.org/stable/plot_types/basic/scatter_plot.html#sphx-glr-plot-types-basic-scatter-plot-py
"""

# Visualizando graficamente
import matplotlib.pyplot as plt

fig, ax = plt.subplots()

ax.scatter(X_train, y_train)

ax.set(yticks=[0,1],xticks=[0,0.3,0.6,0.9,1.2,1.5])

plt.show()

"""- **Utilizando a regressão logística**
    - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression
"""

# Importando a regressão logística
from sklearn.linear_model import LogisticRegression

# Criando o nosso classificador
clf = LogisticRegression(random_state=0).fit(X_train.reshape(-1, 1), y_train)

# Verificando o coeficiente angular
w1 = clf.coef_[0][0]

# E o coeficiente linear
w0 = clf.intercept_[0]

"""- Para traçar esse gráfico, já temos a função `expit` do scipy
    - https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html
    - `expit(x) = 1/(1+exp(-x))`
"""

# Importando o expit
from scipy.special import expit

# Importando o numpy e criando um array de 0 a 2
import numpy as np
valores_x = np.linspace(X_train.min(),X_train.max(),100)
valores_x

"""- Para o caso binário, podemos calcular a curva em função do coef_ e do intercept_ fazendo `expit(x*w1+w0)`
    - https://scikit-learn.org/stable/modules/linear_model.html#binary-case
"""

# Calculando o valor para cada valor de y
valores_y = expit(w1*valores_x+w0)

# Visualizando graficamente
fig, ax = plt.subplots()

ax.scatter(X_train, y_train)
ax.plot(valores_x,valores_y)

ax.set(yticks=[0,1],xticks=[0,0.3,0.6,0.9,1.2,1.5])

plt.show()

# Podemos exibir os dados de treino em cima dessa curva
y_curva = expit(w1*X_train+w0)

# Visualizando graficamente
fig, ax = plt.subplots()

ax.scatter(X_train, y_train)
ax.plot(valores_x,valores_y)
ax.scatter(X_train, y_curva)

ax.axhline(y=0.5,c='m',linestyle='--')

ax.plot([0.1,0.1],[0,0.07883025],'--g')
ax.plot([0.2,0.2],[0,0.11156652],'--g')
ax.plot([0.3,0.3],[0,0.15560101],'--g')
ax.plot([0.4,0.4],[0,0.21285167],'--g')
ax.plot([0.5,0.5],[0,0.28408003],'--g')
ax.plot([0.6,0.6],[0,0.36800066],'--g')
ax.plot([1,1],[0.72972574,1],'--g')
ax.plot([1.1,1.1],[0.79846697,1],'--g')
ax.plot([1.2,1.2],[0.85324086,1],'--g')
ax.plot([1.3,1.3],[0.89508404,1],'--g')
ax.plot([1.4,1.4],[0.92603135,1],'--g')
ax.plot([1.5,1.5],[0.94837655,1],'--g')
ax.plot([1.6,1.6],[0.96423221,1],'--g')

ax.set(yticks=[0,1],xticks=[0,0.3,0.6,0.9,1.2,1.5])

plt.show()

# Fazendo a previsão das probabilidades
clf.predict_proba(X_train.reshape(-1,1))[0:5]

# Verificando o X_train
X_train[0:5]

# e o y_train
y_train[0:5]

"""- **Utilizando essa previsão na base de teste**"""

# Verificando a previsão da probabilidade
y_pred = clf.predict_proba(X_test.reshape(-1,1))

y_pred[:,1]

# Verificando graficamente
fig, ax = plt.subplots()

ax.scatter(X_train, y_train)
ax.plot(valores_x,valores_y)
ax.scatter(X_test,y_pred[:,1])

ax.set(yticks=[0,1],xticks=[0,0.3,0.6,0.9,1.2,1.5])

plt.show()

# E apenas a previsão
y_pred_valor = clf.predict(X_test.reshape(-1,1))

# Avaliando o erro
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,y_pred_valor)

"""- **O caso binomial pode ser estendido para quantas classses quisermos, porém há um aumento na complexidade**
    - https://scikit-learn.org/stable/modules/linear_model.html#binary-case
- **Podemos considerar todos as classes e todas as colunas**
"""

# Importando novamente o dataset
from sklearn.datasets import load_iris
X,y = load_iris(return_X_y = True, as_frame=True)

# Separando em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# Criando novamente o classificador
clf2 = LogisticRegression(random_state=0,max_iter=1000).fit(X_train, y_train)

# Verificando o coef_
clf2.coef_

# e o intercept
clf2.intercept_

# Fazendo a previsão das classes
y_pred2 = clf2.predict(X_test)

# Avaliando o erro
confusion_matrix(y_test,y_pred2)